{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layers': [{'type': 'input', 'neurons': 4}, {'type': 'hidden', 'neurons': 4, 'activation': 'sigmoid'}, {'type': 'output', 'neurons': 2, 'activation': 'sigmoid'}], 'learning_rate': 0.1, 'epochs': 10000, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_json_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Example usage\n",
    "config = read_json_config('MLP.json')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3548416426118447\n",
      "Epoch 100, Loss: 0.211431598655755\n",
      "Epoch 200, Loss: 0.1963710007035473\n",
      "Epoch 300, Loss: 0.18900437780404297\n",
      "Epoch 400, Loss: 0.19141876048889908\n",
      "Epoch 500, Loss: 0.1893230806484545\n",
      "Epoch 600, Loss: 0.18622467348429456\n",
      "Epoch 700, Loss: 0.19149067846320936\n",
      "Epoch 800, Loss: 0.19833123442218845\n",
      "Epoch 900, Loss: 0.20368903087076592\n",
      "Epoch 1000, Loss: 0.20750151163624886\n",
      "Epoch 1100, Loss: 0.21022341720007942\n",
      "Epoch 1200, Loss: 0.21223302323694931\n",
      "Epoch 1300, Loss: 0.21377925214799245\n",
      "Epoch 1400, Loss: 0.21501767886423542\n",
      "Epoch 1500, Loss: 0.2160454502235314\n",
      "Epoch 1600, Loss: 0.21692425585260533\n",
      "Epoch 1700, Loss: 0.21769417559265494\n",
      "Epoch 1800, Loss: 0.2183819029437113\n",
      "Epoch 1900, Loss: 0.2190056714788147\n",
      "Epoch 2000, Loss: 0.219578255184398\n",
      "Epoch 2100, Loss: 0.22010883021986494\n",
      "Epoch 2200, Loss: 0.22060415226526797\n",
      "Epoch 2300, Loss: 0.22106931503817234\n",
      "Epoch 2400, Loss: 0.22150824791735185\n",
      "Epoch 2500, Loss: 0.22192404823570888\n",
      "Epoch 2600, Loss: 0.22231920704788088\n",
      "Epoch 2700, Loss: 0.22269576515049777\n",
      "Epoch 2800, Loss: 0.2230554227169862\n",
      "Epoch 2900, Loss: 0.22339961761039973\n",
      "Epoch 3000, Loss: 0.2237295822285829\n",
      "Epoch 3100, Loss: 0.22404638541946867\n",
      "Epoch 3200, Loss: 0.2243509638638194\n",
      "Epoch 3300, Loss: 0.2246441459228904\n",
      "Epoch 3400, Loss: 0.22492667002120076\n",
      "Epoch 3500, Loss: 0.22519919901259178\n",
      "Epoch 3600, Loss: 0.2254623315553923\n",
      "Epoch 3700, Loss: 0.22571661123226977\n",
      "Epoch 3800, Loss: 0.2259625339485473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eniyan\\AppData\\Local\\Temp\\ipykernel_21856\\1624854878.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3900, Loss: 0.22620055400085678\n",
      "Epoch 4000, Loss: 0.22643108910707654\n",
      "Epoch 4100, Loss: 0.22665452461595775\n",
      "Epoch 4200, Loss: 0.2268712170621294\n",
      "Epoch 4300, Loss: 0.2270814971934879\n",
      "Epoch 4400, Loss: 0.227285672569284\n",
      "Epoch 4500, Loss: 0.2274840298057511\n",
      "Epoch 4600, Loss: 0.22767683652988885\n",
      "Epoch 4700, Loss: 0.2278643430896377\n",
      "Epoch 4800, Loss: 0.22804678405916673\n",
      "Epoch 4900, Loss: 0.2282243795705996\n",
      "Epoch 5000, Loss: 0.22839733649773325\n",
      "Epoch 5100, Loss: 0.22856584951274442\n",
      "Epoch 5200, Loss: 0.22873010203325805\n",
      "Epoch 5300, Loss: 0.2288902670742532\n",
      "Epoch 5400, Loss: 0.22904650801694737\n",
      "Epoch 5500, Loss: 0.22919897930490507\n",
      "Epoch 5600, Loss: 0.22934782707606027\n",
      "Epoch 5700, Loss: 0.22949318973807897\n",
      "Epoch 5800, Loss: 0.22963519849342345\n",
      "Epoch 5900, Loss: 0.22977397781961284\n",
      "Epoch 6000, Loss: 0.22990964590942886\n",
      "Epoch 6100, Loss: 0.230042315075213\n",
      "Epoch 6200, Loss: 0.23017209212086479\n",
      "Epoch 6300, Loss: 0.23029907868471464\n",
      "Epoch 6400, Loss: 0.23042337155606799\n",
      "Epoch 6500, Loss: 0.23054506296788402\n",
      "Epoch 6600, Loss: 0.2306642408677802\n",
      "Epoch 6700, Loss: 0.23078098916930923\n",
      "Epoch 6800, Loss: 0.23089538798523987\n",
      "Epoch 6900, Loss: 0.23100751384440038\n",
      "Epoch 7000, Loss: 0.2311174398934709\n",
      "Epoch 7100, Loss: 0.2312252360849794\n",
      "Epoch 7200, Loss: 0.2313309693526284\n",
      "Epoch 7300, Loss: 0.23143470377496925\n",
      "Epoch 7400, Loss: 0.23153650072834656\n",
      "Epoch 7500, Loss: 0.23163641902994775\n",
      "Epoch 7600, Loss: 0.23173451507171575\n",
      "Epoch 7700, Loss: 0.23183084294581635\n",
      "Epoch 7800, Loss: 0.23192545456228708\n",
      "Epoch 7900, Loss: 0.23201839975944527\n",
      "Epoch 8000, Loss: 0.2321097264075773\n",
      "Epoch 8100, Loss: 0.2321994805063904\n",
      "Epoch 8200, Loss: 0.2322877062766706\n",
      "Epoch 8300, Loss: 0.232374446246545\n",
      "Epoch 8400, Loss: 0.23245974133272734\n",
      "Epoch 8500, Loss: 0.232543630917084\n",
      "Epoch 8600, Loss: 0.23262615291883731\n",
      "Epoch 8700, Loss: 0.23270734386269712\n",
      "Epoch 8800, Loss: 0.2327872389431879\n",
      "Epoch 8900, Loss: 0.23286587208542112\n",
      "Epoch 9000, Loss: 0.2329432760025388\n",
      "Epoch 9100, Loss: 0.2330194822500451\n",
      "Epoch 9200, Loss: 0.23309452127722052\n",
      "Epoch 9300, Loss: 0.23316842247579828\n",
      "Epoch 9400, Loss: 0.2332412142260793\n",
      "Epoch 9500, Loss: 0.23331292394063632\n",
      "Epoch 9600, Loss: 0.23338357810575736\n",
      "Epoch 9700, Loss: 0.23345320232076408\n",
      "Epoch 9800, Loss: 0.23352182133532923\n",
      "Epoch 9900, Loss: 0.23358945908491802\n",
      "Predictions:\n",
      "[[0.45095231 0.55244527]\n",
      " [0.51764021 0.48123634]\n",
      " [0.51764021 0.48123634]\n",
      " [0.51764021 0.48123634]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, config):\n",
    "        self.layers = config['layers']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.epochs = config['epochs']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(1, len(self.layers)):\n",
    "            input_size = self.layers[i-1]['neurons']\n",
    "            output_size = self.layers[i]['neurons']\n",
    "            self.weights.append(np.random.randn(input_size, output_size))\n",
    "            self.biases.append(np.random.randn(1, output_size))\n",
    "\n",
    "    def activate(self, x, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return sigmoid(x)\n",
    "        # Add other activations if needed\n",
    "        return x\n",
    "\n",
    "    def activate_derivative(self, x, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            return sigmoid_derivative(x)\n",
    "        # Add other activations if needed\n",
    "        return x\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        for i in range(len(self.weights)):\n",
    "            net_input = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            activation = self.activate(net_input, self.layers[i+1].get('activation', ''))\n",
    "            activations.append(activation)\n",
    "        return activations\n",
    "\n",
    "    def backward_propagation(self, X, y, activations):\n",
    "        deltas = [y - activations[-1]]\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            delta = deltas[-1] * self.activate_derivative(activations[i+1], self.layers[i+1].get('activation', ''))\n",
    "            deltas.append(delta.dot(self.weights[i].T))\n",
    "        deltas.reverse()\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += activations[i].T.dot(deltas[i+1]) * self.learning_rate\n",
    "            self.biases[i] += np.sum(deltas[i+1], axis=0, keepdims=True) * self.learning_rate\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for epoch in range(self.epochs):\n",
    "            activations = self.forward_propagation(X)\n",
    "            self.backward_propagation(X, y, activations)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.square(y - activations[-1]))\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)[-1]\n",
    "\n",
    "# Load config from JSON file\n",
    "config = read_json_config('MLP.json')\n",
    "\n",
    "# Example input data (X) - 4 samples with 4 features each\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6]\n",
    "])\n",
    "\n",
    "# Example target data (y) - 4 samples with 2 target outputs each\n",
    "y = np.array([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 0]\n",
    "])\n",
    "\n",
    "# Normalize the input data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP(config)\n",
    "\n",
    "# Train the MLP with normalized input data and target data\n",
    "mlp.train(X_normalized, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = mlp.predict(X_normalized)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
